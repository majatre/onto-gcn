{"cells":[{"cell_type":"code","metadata":{"cell_id":"786d5a94-f38e-4934-a9a9-bbfe401cfca2"},"source":"!python --version","execution_count":null,"outputs":[{"name":"stdout","text":"Python 3.7.3\r\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"4ff88351-c5f9-4bc0-84d9-df7e06afa8bb"},"source":"import tensorflow as tf\r\nimport pandas as pd\r\nimport numpy as np\r\nimport matplotlib, matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"ab45db68-fd02-48c6-9ced-e266ad9aa50d"},"source":"def discretize(df, bins=200):\r\n    print(f\"Discretizing features into {bins} bins\")\r\n    result = df.copy()\r\n    for feature_name in df.columns:\r\n        result[feature_name] = pd.cut(df[feature_name], bins=bins, labels=range(bins), include_lowest=True)\r\n        # result[feature_name], _ = pd.factorize(cut_result)\r\n    return result\r\n\r\n\r\ndef normalize(df):\r\n    result = df.copy()\r\n    for feature_name in df.columns:\r\n        max_value = df[feature_name].max()\r\n        min_value = df[feature_name].min()\r\n        if max_value != min_value:\r\n          result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\r\n    return result\r\n\r\ndef load_data(task = 'PAM50', subset='all', discrete=False, bins=200):\r\n    if subset == 'all':\r\n        path = \"/datasets/mbdata/MBdata_all.csv\"\r\n    elif subset == 'original':\r\n        path = \"./MBdata_original.csv\"\r\n    df = pd.read_csv(path)\r\n    if task == 'DR':\r\n        df = df[df.DR != '?']\r\n        target = df.pop('DR')\r\n    elif task == 'ER':\r\n        df = df[df.ER_Status != '?']\r\n        target = df.pop('ER_Status')\r\n        labels = {\r\n            'pos': 0,\r\n            'neg': 1\r\n        }\r\n        target = target.apply(lambda x: labels[x])\r\n    elif task == 'iC10':\r\n        df = df[df.iC10 != '?']\r\n        target = df.pop('iC10')\r\n        labels = {\r\n            '4ER-': 4,\r\n            '4ER+': 0\r\n        }\r\n        target = target.apply(lambda x: labels[x] if x in labels else int(x))\r\n    elif task == 'PAM50':\r\n        df = df[df.Pam50Subtype != '?']\r\n        target = df.pop('Pam50Subtype')\r\n        pam50_lables = {\r\n            'Normal': 0,\r\n            'LumA': 1,\r\n            'LumB': 2,\r\n            'Basal': 3,\r\n            'Her2': 4\r\n        }\r\n        target = target.apply(lambda x: pam50_lables[x])\r\n\r\n\r\n    features = df.filter(regex='^GE.*')\r\n    features = features.astype('float64')\r\n    # print(features.shape)\r\n\r\n    if discrete:\r\n        features = discretize(features, bins)\r\n    else:\r\n        features = normalize(features)\r\n        features.replace([np.inf, -np.inf, np.nan], 0, inplace=True)\r\n    return features, target","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Experiments","metadata":{"tags":[],"cell_id":"0eb1104a-95c4-4d97-a492-69d7e11c5f91"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"892b7538-4bf2-4d6a-9ff0-d9de6d35eb61"},"source":"from sklearn.ensemble import RandomForestClassifier\r\nimport numpy as np\r\nfrom sklearn import metrics\r\nfrom sklearn.model_selection import cross_validate, KFold, StratifiedKFold, train_test_split\r\nimport random\r\n\r\nimport warnings\r\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"194f2a00-4434-4167-9369-88e24ccb8555"},"source":"features, target = load_data(task='DR', subset='original', discrete=True, bins=200)","execution_count":null,"outputs":[{"name":"stdout","text":"Discretizing features into 200 bins\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"1797ef46-60cc-4740-b46a-4cfab23a063d"},"source":"import sklearn.svm\r\nfor bins in [200,100,10,20]:\r\n    features, target = load_data(task='DR', subset='original', discrete=True, bins=bins)\r\n    acc = []\r\n    kf = StratifiedKFold(n_splits=5, shuffle=True)\r\n\r\n    x_train = features.values[:]\r\n    y_train = target.values[:]\r\n\r\n    for C in [0.01, 0.1, 1, 100]:\r\n        for train, valid in kf.split(x_train, y_train):\r\n            svm = sklearn.svm.SVC(kernel='rbf', C=C)\r\n            svm.fit(x_train[train], y_train[train])\r\n            y_pred=svm.predict(x_train[valid])\r\n            acc.append(metrics.accuracy_score(y_train[valid], y_pred))\r\n            print(f\"C={C}: {acc[-1]:.3f}\")\r\n\r\n        print(f\"C={C}: {np.mean(acc):.3f}+-{np.std(acc):.3f}\")","execution_count":null,"outputs":[{"name":"stdout","text":"Discretizing features into 200 bins\nC=0.01: 0.697\nC=0.01: 0.697\nC=0.01: 0.697\nC=0.01: 0.694\nC=0.01: 0.694\nC=0.01: 0.696+-0.001\nC=0.1: 0.697\nC=0.1: 0.697\nC=0.1: 0.697\nC=0.1: 0.694\nC=0.1: 0.694\nC=0.1: 0.696+-0.001\nC=1: 0.699\nC=1: 0.692\nC=1: 0.697\nC=1: 0.694\nC=1: 0.692\nC=1: 0.696+-0.002\nC=100: 0.677\nC=100: 0.672\nC=100: 0.692\nC=100: 0.679\nC=100: 0.659\nC=100: 0.691+-0.010\nDiscretizing features into 100 bins\nC=0.01: 0.697\nC=0.01: 0.697\nC=0.01: 0.697\nC=0.01: 0.694\nC=0.01: 0.694\nC=0.01: 0.696+-0.001\nC=0.1: 0.697\nC=0.1: 0.697\nC=0.1: 0.697\nC=0.1: 0.694\nC=0.1: 0.694\nC=0.1: 0.696+-0.001\nC=1: 0.702\nC=1: 0.694\nC=1: 0.694\nC=1: 0.697\nC=1: 0.697\nC=1: 0.696+-0.002\nC=100: 0.664\nC=100: 0.634\nC=100: 0.682\nC=100: 0.689\nC=100: 0.657\nC=100: 0.689+-0.017\nDiscretizing features into 10 bins\nC=0.01: 0.697\nC=0.01: 0.697\nC=0.01: 0.697\nC=0.01: 0.694\nC=0.01: 0.694\nC=0.01: 0.696+-0.001\nC=0.1: 0.697\nC=0.1: 0.697\nC=0.1: 0.697\nC=0.1: 0.694\nC=0.1: 0.694\nC=0.1: 0.696+-0.001\nC=1: 0.692\nC=1: 0.697\nC=1: 0.697\nC=1: 0.689\nC=1: 0.694\nC=1: 0.695+-0.002\nC=100: 0.682\nC=100: 0.664\nC=100: 0.667\nC=100: 0.682\nC=100: 0.657\nC=100: 0.689+-0.012\nDiscretizing features into 20 bins\nC=0.01: 0.697\nC=0.01: 0.697\nC=0.01: 0.697\nC=0.01: 0.694\nC=0.01: 0.694\nC=0.01: 0.696+-0.001\nC=0.1: 0.697\nC=0.1: 0.697\nC=0.1: 0.697\nC=0.1: 0.694\nC=0.1: 0.694\nC=0.1: 0.696+-0.001\nC=1: 0.694\nC=1: 0.697\nC=1: 0.705\nC=1: 0.694\nC=1: 0.694\nC=1: 0.696+-0.003\nC=100: 0.667\nC=100: 0.657\nC=100: 0.682\nC=100: 0.684\nC=100: 0.659\nC=100: 0.690+-0.013\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"7ab40360-aa7d-4819-970c-1bc1a7b4dd94"},"source":"from sklearn.svm import SVC\r\nfrom sklearn.model_selection import GridSearchCV\r\nfor bins in [200,100,10,20]:\r\n    features, target = load_data(task='DR', subset='original', discrete=True, bins=bins)\r\n    acc_all = []\r\n    kf = StratifiedKFold(n_splits=5, shuffle=True)\r\n\r\n    x_train = features.values[:]\r\n    y_train = target.values[:]\r\n\r\n    for train, valid in kf.split(x_train, y_train):\r\n        params = {'C':[0.001, 0.01, 0.1, 1, 10, 100]}\r\n        classifier = GridSearchCV(SVC(gamma=\"scale\"), params, cv=5, scoring='accuracy', verbose=0, n_jobs=-1)\r\n            \r\n        classifier.fit(x_train[train], y_train[train])\r\n        Y_pred = classifier.predict(x_train[valid])\r\n        acc = metrics.accuracy_score(y_train[valid], Y_pred)\r\n        acc_all.append(acc)\r\n        print(f\"{acc:.3f}\")\r\n\r\n    print(f\"{np.mean(acc):.3f}+-{np.std(acc):.3f}\")","execution_count":null,"outputs":[{"name":"stdout","text":"Discretizing features into 200 bins\n0.689\n0.697\n0.697\n0.697\n0.694\n0.694+-0.000\nDiscretizing features into 100 bins\n0.697\n0.697\n0.697\n0.694\n0.694\n0.694+-0.000\nDiscretizing features into 10 bins\n0.699\n0.697\n0.697\n0.679\n0.694\n0.694+-0.000\nDiscretizing features into 20 bins\n0.702\n0.697\n0.692\n0.694\n0.699\n0.699+-0.000\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"cb7efe52-37e5-4b6c-9df4-1f72ae02c764"},"source":"import sklearn.svm\r\nfeatures, target = load_data(task='PAM50', subset='original', discrete=False, bins=0)\r\nacc = []\r\nkf = StratifiedKFold(n_splits=5, shuffle=True)\r\n\r\nx_train = features.values[:]\r\ny_train = target.values[:]\r\n\r\nfor train, valid in kf.split(x_train, y_train):\r\n    rf = RandomForestClassifier()        \r\n    rf.fit(x_train[train], y_train[train])\r\n    y_pred=rf.predict(x_train[valid])\r\n    acc.append(metrics.accuracy_score(y_train[valid], y_pred))\r\n    print(f\" {acc[-1]:.3f}\")\r\n\r\nprint(f\"{np.mean(acc):.3f}+-{np.std(acc):.3f}\")","execution_count":null,"outputs":[{"name":"stdout","text":" 0.744\n 0.754\n 0.722\n 0.734\n 0.744\n0.740+-0.011\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"0f58d63e-ed2d-4b86-92ed-fa9a82b09194"},"source":"# for bins in [200,100,10,20]:\r\nfeatures, target = load_data(task='PAM50', subset='original', discrete=False, bins=bins)\r\nacc = []\r\n\r\n\r\nfor seed in range(5):\r\n    X_train, X_test, y_train, y_test = train_test_split(features, \r\n                                                        target.to_numpy(), \r\n                                                        stratify=target.to_numpy(),\r\n                                                        train_size=1500,\r\n                                                        test_size=400,\r\n                                                        shuffle=True,\r\n                                                        random_state=seed\r\n                                                        )\r\n    rf = RandomForestClassifier()\r\n    rf.fit(X_train, y_train)\r\n    y_pred=rf.predict(X_test)\r\n    score = metrics.accuracy_score(y_pred, y_test)\r\n    print(score)\r\n    acc.append(score)\r\n\r\nprint(f\"{np.mean(acc):.3f}+-{np.std(acc):.3f}\")","execution_count":null,"outputs":[{"name":"stdout","text":"0.69\n0.6775\n0.6975\n0.7\n0.675\n0.688+-0.010\n0.685\n0.6925\n0.6875\n0.6975\n0.6975\n0.692+-0.005\n0.7025\n0.6925\n0.6925\n0.685\n0.6925\n0.693+-0.006\n0.685\n0.685\n0.6875\n0.7025\n0.6925\n0.691+-0.007\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"40dce9d4-853b-42f3-a35c-21d44ea3dea1"},"source":"for bins in [200,100,10,20]:\r\n    features, target = load_data(task='DR', subset='original', discrete=True, bins=bins)\r\n    acc = []\r\n    features_local = features.copy().astype('float64')\r\n\r\n    # n=1000\r\n    # embeddings='dl2vec'\r\n    # cos_distances_sorted = load_gene_embeddings(name=embeddings, reference_gene='FOXA1')\r\n    # top_genes = ['GE_'+g for g,v in cos_distances_sorted.items() if 'GE_'+g in list(features.columns)]\r\n    # features_local = features_local.filter(top_genes[:n]) #features_o.columns)\r\n\r\n    for seed in range(5):\r\n        X_train, X_test, y_train, y_test = train_test_split(features_local.to_numpy(), \r\n                                                            target.to_numpy(), \r\n                                                            stratify=target.to_numpy(),\r\n                                                            train_size=1000,\r\n                                                            test_size=400,\r\n                                                            shuffle=True,\r\n                                                            random_state=seed\r\n        )\r\n        # y_train = target.values[:]\r\n        b = np.zeros((y_train.size, 11))\r\n        b[np.arange(y_train.size),y_train] = 1\r\n        y_train = b\r\n\r\n        b = np.zeros((y_test.size, 11))\r\n        b[np.arange(y_test.size),y_test] = 1\r\n        y_test = b\r\n\r\n        model = tf.keras.models.Sequential([\r\n            tf.keras.layers.Dense(256, activation='relu'),\r\n            # tf.keras.layers.Dropout(0.2),\r\n            # tf.keras.layers.Dense(256, activation='relu'),\r\n            # tf.keras.layers.Dense(64, activation='relu'),\r\n            tf.keras.layers.Dense(64, activation='relu'),# use_bias=False),\r\n            #tf.keras.layers.BatchNormalization(),\r\n            # tf.keras.layers.Activation(\"relu\"),\r\n            tf.keras.layers.Dense(11, activation='softmax')\r\n        ])\r\n\r\n        adam = tf.keras.optimizers.Adam(learning_rate=0.0001)\r\n        model.compile(optimizer=adam,\r\n                        loss='categorical_crossentropy',\r\n                        metrics=['accuracy'])\r\n\r\n\r\n        model.fit(X_train, y_train, epochs=50,  verbose=0)\r\n        model.evaluate(X_test, y_test, verbose=2)\r\n        y_pred=model.predict(X_test)\r\n        y_pred=np.argmax(y_pred, axis=1)\r\n        score = metrics.accuracy_score(np.argmax(y_test, axis=1), y_pred)\r\n        acc.append(score)\r\n\r\n    print(f\"{np.mean(acc):.3f}+-{np.std(acc):.3f}\")","execution_count":null,"outputs":[{"name":"stdout","text":"Discretizing features into 200 bins\n400/400 - 0s - loss: 4.8633 - accuracy: 0.5925\n400/400 - 0s - loss: 3.0973 - accuracy: 0.6100\n400/400 - 0s - loss: 5.3467 - accuracy: 0.5900\n400/400 - 0s - loss: 8.7673 - accuracy: 0.6000\n400/400 - 0s - loss: 3.8607 - accuracy: 0.5950\n0.597+-0.007\nDiscretizing features into 100 bins\n400/400 - 0s - loss: 2.2859 - accuracy: 0.6100\n400/400 - 0s - loss: 4.0681 - accuracy: 0.6225\n400/400 - 0s - loss: 2.4775 - accuracy: 0.6300\n400/400 - 0s - loss: 2.4045 - accuracy: 0.6325\n400/400 - 0s - loss: 2.3962 - accuracy: 0.6150\n0.622+-0.009\nDiscretizing features into 10 bins\n400/400 - 0s - loss: 1.2557 - accuracy: 0.6400\n400/400 - 0s - loss: 1.1428 - accuracy: 0.6275\n400/400 - 0s - loss: 1.1503 - accuracy: 0.6700\n400/400 - 0s - loss: 1.1213 - accuracy: 0.6825\n400/400 - 0s - loss: 1.0796 - accuracy: 0.7075\n0.665+-0.029\nDiscretizing features into 20 bins\n400/400 - 0s - loss: 1.2637 - accuracy: 0.6625\n400/400 - 0s - loss: 1.2577 - accuracy: 0.6400\n400/400 - 0s - loss: 1.1105 - accuracy: 0.6725\n400/400 - 0s - loss: 1.5973 - accuracy: 0.5925\n400/400 - 0s - loss: 1.1869 - accuracy: 0.6375\n0.641+-0.028\n","output_type":"stream"}]}],"nbformat":4,"nbformat_minor":2,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"},"deepnote_execution_queue":[],"deepnote_notebook_id":"f77a8747-d6c4-4fff-b86e-5ae36d98e725"}}